## Distill mbart For ECK

* Among mbart, I fine-tuned the models for English, Chinese, and Korean. 
* Each model has a different number of layers. (For example, if model denotes 12-3, it represent this model is composed of __12 Encoder__ and __3 Decoder__






